{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBm+gJMO3X6PDW3pbsU8IF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarcelloCeresini/StatMatMethodsAI/blob/main/Lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$X = [x^1 ... x^N]$ --> N initial datapoints of size d\n",
        "we want instead\n",
        "$Z = [z^1 ... z^N]$ ---> N compressed datapoints of size k<<d\n",
        "\n",
        "\n",
        "PCA uses the TRUNCATED SVD (Single Value Decomposition) where: $A_k = \\sum_1^k u_i σ_i v_i^T$\n",
        "\n",
        "We use this kowledge to build a PROJECTION MATRIX, that is simply $U_k^T$ --> $Z = U_k^T X$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2As7mb46Si7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA\n",
        "\n",
        "    1. Take dataset $X$\n",
        "    2. Compute the centroid version $X_c = X - c(X)$ where c(X) is the centroid corresponding to X (mean of coordinates along every direction)\n",
        "    3. Compute the SVD of $X_c$\n",
        "    4. Find $U_k$ given k\n",
        "    5. Compute the new dataset: $Z = U_k^T X$"
      ],
      "metadata": {
        "id": "kyAqWXW2U8ZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"data.csv\")\n",
        "\n",
        "print(\"Shape of data {}\".format(data.shape))\n",
        "\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "msHQbx9URx-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert into array\n",
        "data = np.array(data)\n",
        "\n",
        "# Split into samples and labels (X and Y)\n",
        "\n",
        "X = data[:, 1:]\n",
        "X = X.T\n",
        "\n",
        "Y = data[:, 0]\n",
        "\n",
        "print(X.shape, Y.shape)\n",
        "\n",
        "d, N = X.shape"
      ],
      "metadata": {
        "id": "Jn3AK2pPWrlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize(X, idx):\n",
        "    img = X[:, idx]\n",
        "\n",
        "    img = np.reshape(img, (28, 28))\n",
        "\n",
        "    plt.imshow(img, cmap=\"gray\")\n",
        "    plt.show()\n",
        "\n",
        "idx = 10\n",
        "visualize(X, idx)\n",
        "\n",
        "print(\"The associated digit is {}\".format(Y[idx]))"
      ],
      "metadata": {
        "id": "KiH7KmbMW7YS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(X, Y, Ntrain):\n",
        "    d, N = X.shape\n",
        "\n",
        "    idx = np.arange(N)\n",
        "    np.random.shuffle(idx)\n",
        "\n",
        "    train_idx = idx[:Ntrain]\n",
        "    test_idx = idx[Ntrain:]\n",
        "\n",
        "    Xtrain = X[:, train_idx] # slicing with index array \n",
        "    Ytrain = Y[train_idx]\n",
        "    \n",
        "    Xtest = X[:, test_idx]\n",
        "    Ytest = Y[test_idx]\n",
        "\n",
        "    return (Xtrain, Ytrain), (Xtest, Ytest)\n",
        "\n",
        "# Test it\n",
        "(Xtrain, Ytrain), (Xtest, Ytest) = split_data(X, Y, 30_000)\n",
        "\n",
        "print(Xtrain.shape, Xtest.shape)"
      ],
      "metadata": {
        "id": "n3FyVh4yXc5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SVD of Matrix A\n",
        "\n",
        "```\n",
        "U, S, VT = np.linalg.svd(A, full_matrices=False)\n",
        "```\n",
        "And remember that to take only the first k, you simply impose the last (n-k columns of U to zeros!)\n"
      ],
      "metadata": {
        "id": "oQw_kY6sX9P2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working example to plot clusters\n",
        "THE LABLES HAVE TO BE IN Y as different values"
      ],
      "metadata": {
        "id": "WI3RO0c1YpZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create two cloud of points in 2-dimensional space\n",
        "# and plot it, divided by color\n",
        "x1 = np.random.normal(0, 1, (2, 1000))\n",
        "x2 = np.random.normal(3, 0.1, (2, 100))\n",
        "\n",
        "y1 = np.zeros((1000, ))\n",
        "y2 = np.ones((100, ))\n",
        "\n",
        "# Visualize them (coloring by class)\n",
        "\n",
        "# Join together x1 x2, y1 y2\n",
        "X = np.concatenate((x1, x2), axis=1)\n",
        "Y = np.concatenate((y1, y2))\n",
        "\n",
        "# Visualize\n",
        "plt.scatter(X[0, :], X[1, :], c=Y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vjIEZczPXyNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Discriminant Analysis\n",
        "\n",
        "This is a Supervised Version of PCA, that shifts each column by THE CENTROID OF ITS CLASS instead of the whole dataset's centroid\n",
        "\n",
        "Define K classes and define $I_k = [i_1, ..., i_{N_k}]$ as the VECTOR of INDECES of all the samples in class k (for simplicity we order the initial dataset by class)\n",
        "\n",
        "We define the centroid:\n",
        "\\begin{align}\n",
        "    $c_k(X) = \\frac{1}{N_k} \\sum_{i \\in I_k} x^i $\n",
        "\\end{align}\n",
        "\n",
        "and the global centroid the usual way\n",
        "\n",
        "### Within-cluster\n",
        "CENTERED MATRIX becomes $X_{k,c} = X_k - c_k(X)$\n",
        "\n",
        "Concatenating them all we obtain $ X_w = [X_{1,c} ... X_{K,c}] $\n",
        "\n",
        "THE WITHIN-CLUSTER SCATTER MATRIX is $S_w = X_w X_w^T$ that represents the correlation matrix for points INSIDE each cluster\n",
        "\n",
        "### Between-cluster\n",
        "instead if you just concatenate the $k-th$ centroid $N_k$ times, and then concatenate all of these $K$ matrices together, you have a matrix with repeated centroids (each centroid counted as many times as samples clustered to it) and then shift all by the global centroid --> BETWEEN CLUSTERS SCATTERING MATRIX\n",
        "$S_b = X_{strange} X_{strange}^T$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-NQZFeGCY5aX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LDA: within-clusters distance of the projected data is as small as possible, while the between-clusters distance of the projected data is as big as possible"
      ],
      "metadata": {
        "id": "rB_wzbKbcJfl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want k ORTHONORMAL vectors $q_i$ that create a matrix $Q$, and MAXIMIZE the function $H(q) = \\frac{q^T S_w q}{q^T S_b q}$\n",
        "\n",
        "BUT $H(\\alpha q) = H(q) $ if $\\alpha >0$ --> simply FIX $q^T S_b q=1$ and find $\\max_q q^T S_w q$\n"
      ],
      "metadata": {
        "id": "hBjfh-NYcWN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The implementation simply follows what we just described.\n",
        "\n",
        "Given the input data and the labels:\n",
        "\n",
        "1. Compute $S_w$ and $S_w$ as described above\n",
        "2. If possible, compute the Cholesky decomposition of $S_w = L L^T$\n",
        "3. If not, compute the Cholesky decomposition of $S_w + ϵI = L L^T$ (ϵ ~ 1e-6)\n",
        "4. Find the first $k$ eigenvectors of $L^{-1} S_b L$ and collect them in a matrix $W$\n",
        "5. Compute $Q = L^{-T} W$ and $Q^T$\n",
        "\n",
        "Now $Z = Q^T X$"
      ],
      "metadata": {
        "id": "p1AJZUNxdgLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Convert data into a matrix\n",
        "data = np.array(data)\n",
        "\n",
        "X = data[:, 1:]\n",
        "X = X.T\n",
        "Y = data[:, 0]\n",
        "\n",
        "d, N = X.shape\n",
        "\n",
        "# Find the corresponding indeces\n",
        "I1 = (Y==0)\n",
        "I2 = (Y==6)\n",
        "I3 = (Y==9)\n",
        "\n",
        "# Split X and Y into X1, X2, X3 and Y1, Y2, Y3\n",
        "X1 = X[:, I1]\n",
        "X2 = X[:, I2]\n",
        "X3 = X[:, I3]\n",
        "\n",
        "Y1 = Y[I1]\n",
        "Y2 = Y[I2]\n",
        "Y3 = Y[I3]\n",
        "\n",
        "# Concatenate the data\n",
        "X = np.concatenate((X1, X2, X3), axis=1)\n",
        "Y = np.concatenate((Y1, Y2, Y3))"
      ],
      "metadata": {
        "id": "1QrT9oPAcHjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Within-clusters centroid\n",
        "C1 = np.mean(X1, axis=1)\n",
        "C2 = np.mean(X2, axis=1)\n",
        "C3 = np.mean(X3, axis=1)\n",
        "\n",
        "# Global centroid\n",
        "C = np.mean(X, axis=1)\n"
      ],
      "metadata": {
        "id": "3JQe7QYLe7N-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Center each cluster dataset\n",
        "X1c = X1 - C1.reshape((d, 1))\n",
        "X2c = X2 - C2.reshape((d, 1))\n",
        "X3c = X3 - C3.reshape((d, 1))\n",
        "\n",
        "# Compute the within-cluster matrix by concatenation\n",
        "Xw = np.concatenate((X1c, X2c, X3c), axis=1)\n",
        "\n",
        "# Compute the within-cluster scatter matrix\n",
        "Sw = Xw @ Xw.T"
      ],
      "metadata": {
        "id": "xubFV95Ge-to"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the Xbars\n",
        "Xbar1 = np.repeat(C1.reshape(d, 1), X1.shape[1], axis=1)\n",
        "Xbar2 = np.repeat(C2.reshape(d, 1), X2.shape[1], axis=1)\n",
        "Xbar3 = np.repeat(C3.reshape(d, 1), X3.shape[1], axis=1)\n",
        "\n",
        "# Compute the between-cluster dataset\n",
        "Xbar = np.concatenate((Xbar1, Xbar2, Xbar3), axis=1)\n",
        "\n",
        "# Compute the between-cluster centered dataset\n",
        "Xbarc = Xbar - C.reshape((d, 1))\n",
        "\n",
        "# Compute the between-cluster scatter matrix\n",
        "Sb = Xbarc @ Xbarc.T"
      ],
      "metadata": {
        "id": "xk-tUm5bfEIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We want to compute the Cholesky decomposition of Sw\n",
        "try:\n",
        "    L = np.linalg.cholesky(Sw)\n",
        "except:\n",
        "    epsilon = 1e-6\n",
        "    Sw = Sw + epsilon * np.eye(Sw.shape[0])\n",
        "\n",
        "    L = np.linalg.cholesky(Sw)"
      ],
      "metadata": {
        "id": "f-yCannPfHMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "import scipy.sparse\n",
        "import scipy.sparse.linalg\n",
        "\n",
        "k = 2\n",
        "\n",
        "# Compute the first k eigenvector decomposition of L^-1 Sb L\n",
        "_, W = scipy.sparse.linalg.eigs(np.linalg.inv(L) @ Sb @ L, k=k)\n",
        "W = np.real(W) # should be real but become complex for approximations --> bring them back to real\n",
        "\n",
        "# Compute Q\n",
        "Q = np.linalg.inv(L).T @ W"
      ],
      "metadata": {
        "id": "mWPenw4kfQX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the projection\n",
        "Z = Q.T @ X"
      ],
      "metadata": {
        "id": "UQUSwwC7fgH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification with clustering algorithms\n",
        "\n",
        "1. Compute the projection matrix\n",
        "2. In the PROJECTED SPACE, compute the centroids\n",
        "3. For a NEW datapoint, project it $z = Px$ and classify it with the nearest centroid\n",
        "\n"
      ],
      "metadata": {
        "id": "Gb7pHq4uf9LQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9sNfaAJBf86X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}