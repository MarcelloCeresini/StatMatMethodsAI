{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GD + armijo --> sufficient decrease backtracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def backtracking(f, grad_f, x):\n",
    "    \"\"\"\n",
    "    This function is a simple implementation of the backtracking algorithm for\n",
    "    the GD (Gradient Descent) method.\n",
    "    \n",
    "    Inputs:\n",
    "        - f: function. The function that we want to optimize.\n",
    "        - grad_f: function. The gradient of f(x).\n",
    "        - x: ndarray. The actual iterate x_k.\n",
    "\n",
    "    Output:\n",
    "        - alpha, the step size for the parameters\n",
    "    \"\"\"\n",
    "    alpha = 1\n",
    "    c = 0.8\n",
    "    tau = 0.25\n",
    "    \n",
    "    while f(x - alpha * grad_f(x)) > f(x) - c * alpha * np.linalg.norm(grad_f(x), 2) ** 2:\n",
    "        alpha = tau * alpha\n",
    "        \n",
    "        if alpha < 1e-3:\n",
    "            break\n",
    "    return alpha"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_finished(grad, init_grad, x, previous_x):\n",
    "    tol_grad = 1e-6\n",
    "    first_flag = np.linalg.norm(grad) < (tol_grad * np.linalg.norm(init_grad))\n",
    "\n",
    "    tol_x = 1e-6\n",
    "    second_flag = np.linalg.norm(x-previous_x) < (tol_x * np.linalg.norm(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model that maps $R^n->[0,1]$ --> y_pred = 1 if $f_w(x)>0.5$, 0 otherwise\n",
    "consider a DATASET dxN\n",
    "CONSIDER A LINEAR FUNCTION d+1 (affine transformation, \"matrix\")\n",
    "extend the dataset --> each sample now is $[1, x_1, ..., x_n]$\n",
    "\n",
    "the model is SIGMOID(extended_sample^T LIN_FUNC) --> for each sample this is a scalar in [0, 1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss = MSE! --> %1/2 - (f(x)-y)^2% for each sample\n",
    "gradient becomes: grad(f)^T (f(x)-y) for each sample (now it is a row vector of dimension d+1 because the gradient is for every weight)\n",
    "\n",
    "if we substitute f(x) with sigmoid(x^T w) --> grad(f) = derivative of sigmoid(x^T w) x^T! (still a row vector because it's a scalar by x^T)\n",
    "\n",
    "BUUUT derivative of sigmoid is = sigm(z)(1-sigm(z)) --> grad(f) = sigmoid(x^T w)*(1-sigmoid(x^T w)) x^T\n",
    "\n",
    "so total gradient that was grad(f)^T (f(x)-y) --> sigmoid(x^T w)*(1-sigmoid(x^T w)) x^T * (sigmoid(x^T w)-y) --> for each sample! (can be summed, or better, averaged, to find batch gradient)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define X' extended dataset \n",
    "define f(X) = [f(x) for x in extended dataset]\n",
    "define y = [y for y in labels]\n",
    "\n",
    "then gradient becomes $X'^T sigm(X'^Tw)*(1-sigm(X'^Tw))*(sigm(X'^Tw)-Y)$\n",
    "\n",
    "update: w_new = w_old - alpha/N * gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
