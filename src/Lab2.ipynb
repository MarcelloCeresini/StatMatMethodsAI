{"cells":[{"cell_type":"markdown","metadata":{"id":"2As7mb46Si7d"},"source":["$X = [x^1 ... x^N]$ --> N initial datapoints of size d\n","we want instead\n","$Z = [z^1 ... z^N]$ ---> N compressed datapoints of size k<<d\n","\n","\n","PCA uses the TRUNCATED SVD (Single Value Decomposition) where: $A_k = \\sum_1^k u_i σ_i v_i^T$\n","\n","We use this kowledge to build a PROJECTION MATRIX, that is simply $U_k^T$ --> $Z = U_k^T X$\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kyAqWXW2U8ZD"},"source":["# PCA\n","\n","    1. Take dataset $X$\n","    2. Compute the centroid version $X_c = X - c(X)$ where c(X) is the centroid corresponding to X (mean of coordinates along every direction)\n","    3. Compute the SVD of $X_c$\n","    4. Find $U_k$ given k\n","    5. Compute the new dataset: $Z = U_k^T X$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"msHQbx9URx-8"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","data = pd.read_csv(\"data.csv\")\n","\n","print(\"Shape of data {}\".format(data.shape))\n","\n","print(data.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jn3AK2pPWrlt"},"outputs":[],"source":["# Convert into array\n","data = np.array(data)\n","\n","# Split into samples and labels (X and Y)\n","\n","X = data[:, 1:]\n","X = X.T\n","\n","Y = data[:, 0]\n","\n","print(X.shape, Y.shape)\n","\n","d, N = X.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KiH7KmbMW7YS"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def visualize(X, idx):\n","    img = X[:, idx]\n","\n","    img = np.reshape(img, (28, 28))\n","\n","    plt.imshow(img, cmap=\"gray\")\n","    plt.show()\n","\n","idx = 10\n","visualize(X, idx)\n","\n","print(\"The associated digit is {}\".format(Y[idx]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n3FyVh4yXc5e"},"outputs":[],"source":["def split_data(X, Y, Ntrain):\n","    d, N = X.shape\n","\n","    idx = np.arange(N)\n","    np.random.shuffle(idx)\n","\n","    train_idx = idx[:Ntrain]\n","    test_idx = idx[Ntrain:]\n","\n","    Xtrain = X[:, train_idx] # slicing with index array \n","    Ytrain = Y[train_idx]\n","    \n","    Xtest = X[:, test_idx]\n","    Ytest = Y[test_idx]\n","\n","    return (Xtrain, Ytrain), (Xtest, Ytest)\n","\n","# Test it\n","(Xtrain, Ytrain), (Xtest, Ytest) = split_data(X, Y, 30_000)\n","\n","print(Xtrain.shape, Xtest.shape)"]},{"cell_type":"markdown","metadata":{"id":"oQw_kY6sX9P2"},"source":["### SVD of Matrix A\n","\n","```\n","U, S, VT = np.linalg.svd(A, full_matrices=False)\n","```\n","And remember that to take only the first k, you simply impose the last (n-k columns of U to zeros!)\n"]},{"cell_type":"markdown","metadata":{"id":"WI3RO0c1YpZd"},"source":["# Working example to plot clusters\n","THE LABLES HAVE TO BE IN Y as different values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vjIEZczPXyNJ"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Create two cloud of points in 2-dimensional space\n","# and plot it, divided by color\n","x1 = np.random.normal(0, 1, (2, 1000))\n","x2 = np.random.normal(3, 0.1, (2, 100))\n","\n","y1 = np.zeros((1000, ))\n","y2 = np.ones((100, ))\n","\n","# Visualize them (coloring by class)\n","\n","# Join together x1 x2, y1 y2\n","X = np.concatenate((x1, x2), axis=1)\n","Y = np.concatenate((y1, y2))\n","\n","# Visualize\n","plt.scatter(X[0, :], X[1, :], c=Y)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"-NQZFeGCY5aX"},"source":["# Linear Discriminant Analysis\n","\n","This is a Supervised Version of PCA, that shifts each column by THE CENTROID OF ITS CLASS instead of the whole dataset's centroid\n","\n","Define K classes and define $I_k = [i_1, ..., i_{N_k}]$ as the VECTOR of INDECES of all the samples in class k (for simplicity we order the initial dataset by class)\n","\n","We define the centroid:\n","\\begin{align}\n","    $c_k(X) = \\frac{1}{N_k} \\sum_{i \\in I_k} x^i $\n","\\end{align}\n","\n","and the global centroid the usual way\n","\n","### Within-cluster\n","CENTERED MATRIX becomes $X_{k,c} = X_k - c_k(X)$\n","\n","Concatenating them all we obtain $ X_w = [X_{1,c} ... X_{K,c}] $\n","\n","THE WITHIN-CLUSTER SCATTER MATRIX is $S_w = X_w X_w^T$ that represents the correlation matrix for points INSIDE each cluster\n","\n","### Between-cluster\n","instead if you just concatenate the $k-th$ centroid $N_k$ times, and then concatenate all of these $K$ matrices together, you have a matrix with repeated centroids (each centroid counted as many times as samples clustered to it) and then shift all by the global centroid --> BETWEEN CLUSTERS SCATTERING MATRIX\n","$S_b = X_{strange} X_{strange}^T$\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rB_wzbKbcJfl"},"source":["LDA: within-clusters distance of the projected data is as small as possible, while the between-clusters distance of the projected data is as big as possible"]},{"cell_type":"markdown","metadata":{"id":"hBjfh-NYcWN1"},"source":["We want k ORTHONORMAL vectors $q_i$ that create a matrix $Q$, and MAXIMIZE the function $H(q) = \\frac{q^T S_w q}{q^T S_b q}$\n","\n","BUT $H(\\alpha q) = H(q) $ if $\\alpha >0$ --> simply FIX $q^T S_b q=1$ and find $\\max_q q^T S_w q$\n"]},{"cell_type":"markdown","metadata":{"id":"p1AJZUNxdgLc"},"source":["The implementation simply follows what we just described.\n","\n","Given the input data and the labels:\n","\n","1. Compute $S_w$ and $S_w$ as described above\n","2. If possible, compute the Cholesky decomposition of $S_w = L L^T$\n","3. If not, compute the Cholesky decomposition of $S_w + ϵI = L L^T$ (ϵ ~ 1e-6)\n","4. Find the first $k$ eigenvectors of $L^{-1} S_b L$ and collect them in a matrix $W$\n","5. Compute $Q = L^{-T} W$ and $Q^T$\n","\n","Now $Z = Q^T X$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1QrT9oPAcHjV"},"outputs":[],"source":["data = pd.read_csv('data.csv')\n","\n","# Convert data into a matrix\n","data = np.array(data)\n","\n","X = data[:, 1:]\n","X = X.T\n","Y = data[:, 0]\n","\n","d, N = X.shape\n","\n","# Find the corresponding indeces\n","I1 = (Y==0)\n","I2 = (Y==6)\n","I3 = (Y==9)\n","\n","# Split X and Y into X1, X2, X3 and Y1, Y2, Y3\n","X1 = X[:, I1]\n","X2 = X[:, I2]\n","X3 = X[:, I3]\n","\n","Y1 = Y[I1]\n","Y2 = Y[I2]\n","Y3 = Y[I3]\n","\n","# Concatenate the data\n","X = np.concatenate((X1, X2, X3), axis=1)\n","Y = np.concatenate((Y1, Y2, Y3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3JQe7QYLe7N-"},"outputs":[],"source":["# Within-clusters centroid\n","C1 = np.mean(X1, axis=1)\n","C2 = np.mean(X2, axis=1)\n","C3 = np.mean(X3, axis=1)\n","\n","# Global centroid\n","C = np.mean(X, axis=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xubFV95Ge-to"},"outputs":[],"source":["# Center each cluster dataset\n","X1c = X1 - C1.reshape((d, 1))\n","X2c = X2 - C2.reshape((d, 1))\n","X3c = X3 - C3.reshape((d, 1))\n","\n","# Compute the within-cluster matrix by concatenation\n","Xw = np.concatenate((X1c, X2c, X3c), axis=1)\n","\n","# Compute the within-cluster scatter matrix\n","Sw = Xw @ Xw.T"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xk-tUm5bfEIS"},"outputs":[],"source":["# Compute the Xbars\n","Xbar1 = np.repeat(C1.reshape(d, 1), X1.shape[1], axis=1)\n","Xbar2 = np.repeat(C2.reshape(d, 1), X2.shape[1], axis=1)\n","Xbar3 = np.repeat(C3.reshape(d, 1), X3.shape[1], axis=1)\n","\n","# Compute the between-cluster dataset\n","Xbar = np.concatenate((Xbar1, Xbar2, Xbar3), axis=1)\n","\n","# Compute the between-cluster centered dataset\n","Xbarc = Xbar - C.reshape((d, 1))\n","\n","# Compute the between-cluster scatter matrix\n","Sb = Xbarc @ Xbarc.T"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f-yCannPfHMZ"},"outputs":[],"source":["# We want to compute the Cholesky decomposition of Sw\n","try:\n","    L = np.linalg.cholesky(Sw)\n","except:\n","    epsilon = 1e-6\n","    Sw = Sw + epsilon * np.eye(Sw.shape[0])\n","\n","    L = np.linalg.cholesky(Sw)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mWPenw4kfQX-"},"outputs":[],"source":["import scipy\n","import scipy.sparse\n","import scipy.sparse.linalg\n","\n","k = 2\n","\n","# Compute the first k eigenvector decomposition of L^-1 Sb L\n","_, W = scipy.sparse.linalg.eigs(np.linalg.inv(L) @ Sb @ L, k=k)\n","W = np.real(W) # should be real but become complex for approximations --> bring them back to real\n","\n","# Compute Q\n","Q = np.linalg.inv(L).T @ W"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UQUSwwC7fgH2"},"outputs":[],"source":["# Compute the projection\n","Z = Q.T @ X"]},{"cell_type":"markdown","metadata":{"id":"Gb7pHq4uf9LQ"},"source":["# Classification with clustering algorithms\n","\n","1. Compute the projection matrix\n","2. In the PROJECTED SPACE, compute the centroids\n","3. For a NEW datapoint, project it $z = Px$ and classify it with the nearest centroid\n","\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPimh8bZG8KvmNKWU0q188p","provenance":[{"file_id":"https://github.com/MarcelloCeresini/StatMatMethodsAI/blob/main/Lab2.ipynb","timestamp":1673711902194}]},"kernelspec":{"display_name":"Python 3.10.6 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.6"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":0}
