{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1673711124312,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"mvg3XkGt46Ul"},"outputs":[{"data":{"text/plain":["'/home/franchetto4/github/StatMatMethodsAI'"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import scipy\n","import scipy.sparse\n","import scipy.sparse.linalg\n","import matplotlib.pyplot as plt\n","\n","ROOT_PATH = os.path.normpath(os.path.join(os.getcwd(), os.pardir))\n","ROOT_PATH"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3522,"status":"ok","timestamp":1673711127826,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"8eOQx_GXsMcp","outputId":"fe86b962-bc98-4498-c052-104cfef12e64"},"outputs":[],"source":["try:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    data = pd.read_csv(\"/content/drive/MyDrive/data.csv\")\n","except:\n","    data = pd.read_csv(os.path.join(ROOT_PATH, \"data\", \"data.csv\"))\n","# Load the data\n","\n","print(\"Shape of data {}\".format(data.shape))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1673711127827,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"HydocDcBtHE3","outputId":"aa7da9b7-3196-4f0c-fe04-1caf54bd3264"},"outputs":[],"source":["print(data.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1673711127827,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"Sgcra9Vrtfmr","outputId":"7607f3c5-def5-4598-c851-8d1074f6d69a"},"outputs":[],"source":["# Convert into array\n","data = np.array(data)\n","\n","# Split into samples and labels (X and Y)\n","X = data[:, 1:]\n","X = X.T\n","\n","Y = data[:, 0]\n","\n","print(X.shape, Y.shape)\n","\n","d, N = X.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1673711128180,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"OnPjfzymttxL"},"outputs":[],"source":["def get_data_from_index(X, Y, indexes):\n","    \n","    # start from empty lists for both samples and labels\n","    final_X = []\n","    final_Y = []\n","\n","    # for each chosen label\n","    for k in indexes:\n","        # find which samples have label=k\n","        idxs_k = (Y == k)\n","        # slice the samples and append them to a list\n","        final_X.append( X[:, idxs_k] )\n","        # same thing to the labels\n","        final_Y.append( Y[idxs_k] )\n","\n","    # concatenate together all the previous iterations\n","    X = np.concatenate(final_X, axis=1)\n","    Y = np.concatenate(final_Y)\n","\n","    # return the new dataset and labels\n","    return X, Y"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1673711128180,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"k4saNLCGwArd","outputId":"54463b87-9817-4a50-c6e5-1d20e1812343"},"outputs":[],"source":["indeces = [0, 6, 9]\n","X, Y = get_data_from_index(X, Y, indeces)\n","print(\"X shape: {}, Y shape: {}\".format(\n","    X.shape,\n","    Y.shape\n","))\n","\n","print(\"Kept samples choosing indeces {}: {:.2f}%\".format(\n","    indeces,\n","    X.shape[1]/N*100\n","))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1673711128181,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"_3lWSvn4wGRJ"},"outputs":[],"source":["# 80% to train, the rest to test\n","train_split = 0.8\n","N_train = round(X.shape[1]*train_split)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1673711128182,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"uvdUBrAaxP5e"},"outputs":[],"source":["def data_splits(X, Y, N_train):\n","    N = X.shape[1]\n","\n","    # getting an array with indeces from 0 to N-1\n","    indeces = np.arange(N)\n","    # shuffling randomly\n","    np.random.shuffle(indeces)\n","\n","    # get the first N_train for the train split (but now they are random)\n","    train_idx = indeces[:N_train]\n","    # the rest are for test split\n","    test_idx = indeces[N_train:]\n","\n","    # slice the original datasets with an index array\n","    X_train = X[:, train_idx]  \n","    Y_train = Y[train_idx]\n","    \n","    X_test = X[:, test_idx]\n","    Y_test = Y[test_idx]\n","\n","    # put in tuples the two splits\n","    return (X_train, Y_train), (X_test, Y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1673711128182,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"YB1OMS31x4kX","outputId":"8e6cbde1-be63-45e9-e11b-17191f674385"},"outputs":[],"source":["# get the train and test splits both for samples and for lables\n","(X_train, Y_train), (X_test, Y_test) = data_splits(X, Y, N_train)\n","\n","print(\"X_train shape: {}, Y_train shape: {}\".format(\n","    X_train.shape,\n","    Y_train.shape\n","))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1673711128183,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"5pFf5Slex8kX"},"outputs":[],"source":["# set the dimensionality of the reduction\n","k = 2"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1673711128183,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"GQLh0dGkyBya"},"outputs":[],"source":["def PCA(X, k):\n","\n","    # Find the centroid of the dataset\n","    centroid = np.mean(X, axis=1)\n","\n","    # Translate the whole dataset so that its center is in 0\n","    X_c = X - centroid.reshape((d, 1))\n","\n","    # Compute SVD on the shifted dataset matrix\n","    U, S, VT = np.linalg.svd(X_c, full_matrices=False)\n","\n","    # Take only the first k columns on the U matrix: this is now the projection matrix for the PCA\n","    U_k = U[:, :k]\n","\n","    # print(\"Projection matrix shape: {}\".format(U_k.shape))\n","\n","    # Transpose the projection matrix and apply it to the dataset\n","    return U_k.T @ X"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4343,"status":"ok","timestamp":1673711132518,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"giipm90s1xmx","outputId":"998408dc-9f8c-4b8e-cc06-7fe6458508ec"},"outputs":[],"source":["X_PCA_train = PCA(X_train, k)\n","print(\"Projected dataset shape: {}\".format(X_PCA_train.shape))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1673711132519,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"Z64Rt3ym0tZo"},"outputs":[],"source":["def LDA(X, Y, k):\n","    # get unique label values\n","    unique_idxs = np.unique(Y)\n","    \n","    # create the clusters divided by class\n","    clusters = []\n","    for i in unique_idxs:\n","        cluster = X[:, (Y==i)]\n","        clusters.append(cluster)\n","\n","    # start with constructing the WITHIN-CLUSTER scatter matrix\n","    # compute the centroids for each cluster\n","    centroids = [np.mean(cluster, axis=1) for cluster in clusters]\n","    # shift each cluster by their centroid so their center is in 0\n","    shifted_clusters = [cluster - centroid.reshape((d, 1)) \n","        for cluster, centroid in zip(clusters, centroids)]\n","    # concatenate the shifted clusters\n","    Xw = np.concatenate(shifted_clusters, axis=1)\n","    # compute the within-cluster scatter matrix (how far is each sample from its centroid, more or less)\n","    Sw = Xw @ Xw.T\n","\n","    # second step: construction of the BETWEEN-CLUSTER scatter matrix\n","    # repeat each centroid as many times as the number of samples in their cluster\n","    repeated_centroids = [np.repeat(centroid.reshape(d, 1), cluster.shape[1], axis=1)\n","        for cluster, centroid in zip(clusters, centroids)]\n","    # concatenate them all\n","    Xbar = np.concatenate(repeated_centroids, axis=1)\n","\n","    # find the global centroid of the data\n","    global_centroid = np.mean(X, axis=1)\n","    # shift the \"repeated centroids matrix\" by the global centroid\n","    Xbarc = Xbar - global_centroid.reshape((d, 1))\n","    # compute the between-cluster scatter matrix (how far is each centroid from the global one, more or less)\n","    Sb = Xbarc @ Xbarc.T\n","\n","    try:\n","        # if the within-cluster scatter matrix is SPD, compute its cholesky decomposition\n","        L = np.linalg.cholesky(Sw)\n","    except:\n","        # otherwise, add a small perturbation in the form of the identity matrix\n","        epsilon = 1e-6\n","        # this shifts the eigenvalues to the right by epsilon\n","        # REMARK: for any matrix X, X@X^T is SPD (x^T @ A @ x >= 0), so only numerical error can make it non SPD\n","        # this is why epsilon is enough to bring it back to SPD\n","        Sw = Sw + epsilon * np.eye(Sw.shape[0])\n","\n","        # once it is SPD, compute its cholesky decomposition\n","        L = np.linalg.cholesky(Sw)\n","\n","    # Compute the first k eigenvector decomposition of L^-1 @ Sb @ L\n","    _, W = scipy.sparse.linalg.eigs(np.linalg.inv(L) @ Sb @ L, k=k)\n","    # Sb should be SPD and L^-1 @ Sb @ L is just a change of basis, so its eigenvalues should remain all POSITIVE and REAL\n","    # but numerical errors can add an imaginary component, so we assume that the latter is small and take only the real component\n","    W = np.real(W)\n","    \n","    # Compute Q, the projection matrix of LDA \n","    Q = np.linalg.inv(L).T @ W\n","\n","    # print(\"Projection matrix shape: {}\".format(Q.shape))\n","\n","    # Compute the LDA projection on the initial dataset\n","    return Q.T @ X\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1309,"status":"ok","timestamp":1673711133804,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"fa55HwvF9B1C","outputId":"14087647-5281-42d9-f80f-47845164790d"},"outputs":[],"source":["X_LDA_train = LDA(X_train, Y_train, k)\n","print(\"Projected dataset shape: {}\".format(X_LDA_train.shape))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_clusters_2D(X, Y):\n","    scatter = plt.scatter(X[0, :], X[1, :], c=Y)\n","    plt.legend(handles=scatter.legend_elements()[0], labels=[str(y) for y in np.unique(Y)])\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_clusters_2D(X_PCA_train, Y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_clusters_2D(X_LDA_train, Y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1673711686074,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"JSFuIFj69K1Z"},"outputs":[],"source":["def avg_distance_centroids(X, Y):\n","    # get unique label values\n","    unique_idxs = np.unique(Y)\n","    \n","    # create the clusters divided by class\n","    distances = []\n","    centroids = []\n","    for i in unique_idxs:\n","        cluster = X[:, (Y==i)]\n","        d, N = cluster.shape\n","        centroid = np.mean(cluster, axis=1)\n","        print(\"Coordinates of centroid corresponding to index {}: [{:.2f}, {:.2f}]\".format(\n","            i,\n","            centroid[0],\n","            centroid[1]\n","        ))\n","\n","        distances.append([np.linalg.norm(cluster[:,j] - centroid) for j in range(N)])\n","        centroids.append(centroid)\n","\n","    global_centroid = np.mean(X, axis=1)\n","    print(\"Coordinates of global centroid: [{:.2f}, {:.2f}]\".format(\n","        global_centroid[0],\n","        global_centroid[1]\n","    ))\n","\n","    centroid_avg_distance_to_global = np.mean([np.linalg.norm(c-global_centroid) for c in centroids])\n","    return np.mean(np.concatenate(distances)), centroid_avg_distance_to_global\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":353,"status":"ok","timestamp":1673711686789,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"fy0CTBiu-kuN","outputId":"dd9d3e0d-4977-41ce-d9c0-9519a955a6f6"},"outputs":[],"source":["distance_PCA, avg_centroid_distance = avg_distance_centroids(X_PCA_train, Y_train)\n","print(\"Average distance from point to corresponding centroid with PCA: {:.2f}\".format(distance_PCA))\n","print(\"After normalization by average centroid distance: {:.2f}\".format(distance_PCA / avg_centroid_distance))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":223,"status":"ok","timestamp":1673711695458,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"2aJl6XL2-pe_","outputId":"6ddb9f98-8f85-4253-8035-591274c1ef70"},"outputs":[],"source":["distance_LDA, avg_centroid_distance = avg_distance_centroids(X_LDA_train, Y_train)\n","print(\"Average distance from point to corresponding centroid with PCA: {:.2f}\".format(distance_LDA))\n","print(\"After normalization by average centroid distance: {:.2f}\".format(distance_LDA / avg_centroid_distance))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1483,"status":"ok","timestamp":1673711764099,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"h1ybw5zp_f3k","outputId":"58b2266e-7cce-484c-ab7b-e59c34cae092"},"outputs":[],"source":["X_PCA_test = PCA(X_test, k)\n","distance_PCA_test, avg_centroid_distance = avg_distance_centroids(X_PCA_test, Y_test)\n","print(\"Average distance from point to corresponding centroid with PCA: {:.2f}\".format(distance_PCA_test))\n","print(\"After normalization by average centroid distance: {:.2f}\".format(distance_PCA_test / avg_centroid_distance))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":682,"status":"ok","timestamp":1673711786707,"user":{"displayName":"","userId":""},"user_tz":-60},"id":"XvawfB3L_gMp","outputId":"f690cabb-c374-4023-d13c-7ef211d5a1cf"},"outputs":[],"source":["X_LDA_test = LDA(X_test, Y_test, k)\n","distance_LDA_test, avg_centroid_distance = avg_distance_centroids(X_LDA_test, Y_test)\n","print(\"Average distance from point to corresponding centroid with PCA: {:.2f}\".format(distance_LDA_test))\n","print(\"After normalization by average centroid distance: {:.2f}\".format(distance_LDA_test / avg_centroid_distance))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Explanation of previous results\n","\n","PCA only projects the initial datapoint on their main components given by the eigenvectors of the reduced $U_k$ matrix.\n","\n","LDA instead takes into consideration both their intra-cluster covariance and also their between-cluster covariance when computing Sb and Sw, so the datapoints are not only projected on their main components, but also \"normalized\".\n","\n","This explains the enormous difference between the PCA average distance of ~500 and the small ~0.01 distance for LDA. When normalize the average distance point-centroid with the average distance centroid-global_centroid, PCA returns ~0.8 while LDA returns ~0.3, meaning that the two algorithms actually behave similarly. However, LDA still produces a lower number, meaning that, on average, the points are nearer to their centroid, fact that is very clear also through the plots."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def classify_sample(sample, centroids, classes):\n","    distances = [np.linalg.norm(sample-c) for c in centroids]\n","    return classes[np.argmin(distances)]\n","\n","def get_accuracy(X_train, Y_train, X_test, Y_test):\n","    # get unique label values\n","    classes = np.unique(Y_train)\n","    \n","    centroids = []\n","    # create the clusters divided by class\n","    for i in classes:\n","        cluster = X_train[:, (Y_train==i)]\n","        centroids.append(np.mean(cluster, axis=1))\n","\n","    correct_predictions = 0\n","    for sample, gt_class in zip(X_test.T, Y_test):\n","        predicted_class = classify_sample(sample, centroids, classes)\n","        if predicted_class == gt_class:\n","            correct_predictions += 1\n","\n","    return correct_predictions / len(Y_test)    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"PCA accuracy on test set: {:.2f}%\".format(get_accuracy(X_PCA_train, Y_train, X_PCA_test, Y_test)*100))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"LDA accuracy on test set: {:.2f}%\".format(get_accuracy(X_LDA_train, Y_train, X_LDA_test, Y_test)*100))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def complete_experiment(data, digits, k, train_split):\n","    X = data[:, 1:]\n","    X = X.T\n","    Y = data[:, 0]\n","    \n","    X, Y = get_data_from_index(X, Y, digits)\n","\n","    N_train = round(X.shape[1]*train_split)\n","\n","    (X_train, Y_train), (X_test, Y_test) = data_splits(X, Y, N_train)\n","\n","    X_PCA_train = PCA(X_train, k)\n","    X_LDA_train = LDA(X_train, Y_train, k)\n","\n","    X_PCA_test = PCA(X_test, k)\n","    X_LDA_test = LDA(X_test, Y_test, k)\n","\n","    acc_PCA = get_accuracy(X_PCA_train, Y_train, X_PCA_test, Y_test)\n","    acc_LDA = get_accuracy(X_LDA_train, Y_train, X_LDA_test, Y_test)\n","\n","    return acc_PCA, acc_LDA"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["k_values = np.arange(2,103,10)\n","results = [complete_experiment(data, [0, 6, 9], k, 0.8) for k in k_values]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["k_values = np.arange(2,103,10)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(k_values, [r[0] for r in results], k_values,  [r[1] for r in results])\n","plt.legend([\"PCA\", \"LDA\"])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["digit_values = [\n","    [0, 6, 9],\n","    [2, 5],\n","    [8, 4, 7, 3],\n","    # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n","    [1, 7]\n","]\n","\n","results = [complete_experiment(data, d, 2, 0.8) for d in digit_values]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x_range = np.arange(0, len(digit_values))\n","plt.plot(x_range, [r[0] for r in results], x_range, [r[1] for r in results])\n","plt.xticks(digit_values)\n","plt.legend([\"PCA\", \"LDA\"])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOLuvwnQ4UvUt6kexmUebVP","mount_file_id":"https://github.com/MarcelloCeresini/StatMatMethodsAI/blob/main/Homework2.ipynb","provenance":[{"file_id":"https://github.com/MarcelloCeresini/StatMatMethodsAI/blob/main/Homework2.ipynb","timestamp":1673712030137}]},"kernelspec":{"display_name":"Python 3.10.6 ('env': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"},"vscode":{"interpreter":{"hash":"31e027379c6121b5e6bf64ed9100277294218b9e98ce7db2c95bb2476df4c3fc"}}},"nbformat":4,"nbformat_minor":0}
